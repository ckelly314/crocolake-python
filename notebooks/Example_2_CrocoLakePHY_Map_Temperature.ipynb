{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffcc73d-56f7-41e3-9d61-e01ab4d8f761",
   "metadata": {},
   "source": [
    "### Example 2 - Temperature map in the North West Atlantic\n",
    "\n",
    "This example shows how to read and manipulate CrocoLake data stored in parquet format. The data are stored across multiple files: we will load into memory only what we need by applying some filters, and we will create a map showing the temperature measurements in the North West Atlantic.\n",
    "\n",
    "CrocoLake contains QCed data from different datasets (Argo, GLODAP, Spray Gliders as of today). Example 1 shows how to use the fully Argo data.\n",
    "\n",
    "##### Note on parquet files\n",
    "There are several ways to load parquet files in a dataframe in Python, and this notebook shows two of them:\n",
    "\n",
    "1. pandas + pyarrow: this approach uses the pyarrow package to load the data into a pandas dataframe;\n",
    "2. dask (+ pandas + pyarrow): this approach uses the dask package to load the data into a dask dataframe; it uses pandas and pyarrow under the hood, and a dask dataframe is (almost) identical to a pandas dataframe.\n",
    "\n",
    "We will first use pyarrow to load the dataset, so that we can provide a target schema (containing the Argo variable names) and load the data consistently across floats, independently of what variables each float actually has. We will finally convert the dataset to a pandas dataframe for manipulation and plots.\n",
    "\n",
    "Other ways to read parquet files are by using pandas (make sure you have pyarrow, fastparquet or some other suitable engine installed), and Dask. Generally speaking, you'll want to use Dask only if you need a large amount of data at the same time so that you can benefit from its parallelization. You should avoid Dask whenever the data fits in your RAM.\n",
    "\n",
    "When reading parquet files with pyarrow (or pandas), you can either specificy the file name(s), or the directory containing all the parquet files. In latter case if you apply any filter, pandas and pyarrow will sort through all the files in the folder, reading into memory only the subsets that satisfy your filter.\n",
    "\n",
    "#### Getting started\n",
    "\n",
    "If you haven't already, install the required packages by running `pip install .` at the root of the repository.\n",
    "\n",
    "We also need the dataset! In this example we use the CrocoLake PHY dataset: you can uncomment and run the cell below, or copy-paste the command (without the leading `!`) in your command line.\n",
    "\n",
    "The script downloads the dataset to the default directory `./CrocoLake`; if you want to specify a different path, you can use the `--destination` flag.\n",
    "\n",
    "NB: the download might take a while, the PHY datasets are ~20GB and the BGC ~6GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8de76-071d-43e6-9fc5-e9470398055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !download_db -d CrocoLake -t PHY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d38a1-3526-47aa-acca-d2b64199089c",
   "metadata": {},
   "source": [
    "We then import the necessary modules and set up the path to the dataset (update the `parquet_dir` variable below if you have specified a different location in the previous cell or have moved the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f1b33-cc9b-4b38-9fc1-7880c4083901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Path to CrocoLake PHY\n",
    "parquet_dir = './CrocoLake/0007_PHY_CROCOLAKE-QC-MERGED/'\n",
    "# Setting up parquet schema\n",
    "crocolake_schema = pq.read_schema(parquet_dir+\"_common_metadata\")\n",
    "\n",
    "# Geographical boundaries\n",
    "lat0 = 30\n",
    "lat1 = 45\n",
    "lon0 = -85\n",
    "lon1 = -60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3b16c-18e5-4904-8422-8f17edb48ef9",
   "metadata": {},
   "source": [
    "### pyarrow + pandas approach\n",
    "\n",
    "We now create a `ParquetDataset` object that will allow us to read the data. Specifying a schema (that we read into `crocolake_schema` in the previous cell) will make later operations faster.\n",
    "\n",
    "We can get a peak at what variables are available in the dataset looking at its `schema` attribute. Note that we have not load any data into memory yet (except for the schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f037968-1d68-4c88-83be-d8daee33c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pq.ParquetDataset(\n",
    "    parquet_dir, \n",
    "    schema=crocolake_schema\n",
    ")\n",
    "schema = dataset.schema\n",
    "pprint(sorted(schema.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7615305-d32b-4b1e-84f0-2f4cec304595",
   "metadata": {},
   "source": [
    "We now want to filter the dataset to load in memory only the data from the NWA (we set the values for `lat0`,`lat1`,`lon0`,`lon1` in the first cell) and recorded in the last 6 months.\n",
    "\n",
    "The geographical coordinates are stored in the variables 'LATITUDE'and 'LONGITUDE'. We then generate the filter, with its syntax being: `[[(column, op, val), …],…]` where `column` is the variable name, and `val` is the value to for the operator `op`, which accepts `[==, =, >, >=, <, <=, !=, in, not in]`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af307b-f887-4d68-90a9-c0defad34f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "date0 = datetime.datetime(2010, 1, 1, 0, 0, 0)\n",
    "date1 = datetime.datetime(2020, 1, 1, 0, 0, 0)\n",
    "\n",
    "filter_coords_time = [\n",
    "    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n",
    "    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n",
    "    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcee75-41f9-4b78-8142-5a9eed690ba7",
   "metadata": {},
   "source": [
    "To get a pandas dataframe, we re-generate a `ParquetDataset` object adding the filters to it, and then we read it into a pandas dataframe with the `read().to_pandas()` methods of the dataset.\n",
    "\n",
    "*NB:* The following operation fetches a large amount of data (~3.6 GB), so you can reduce the number of days in filter above if you cannot use this much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1ccaa-b68a-4baf-9558-52eef5a7d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = pq.ParquetDataset(parquet_dir, schema=crocolake_schema, filters=filter_coords_time)\n",
    "df = ds.read().to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653498a-afd1-4002-8951-82206a8f1660",
   "metadata": {},
   "source": [
    "You can explore the dataframe just by calling it (`df`) as we did above. If you want a list of the variables that are stored, you can use `sorted(df.columns.to_list())`. The following cell shows that we loaded roughly 2 GB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3eb63-11cc-4c94-af5a-f4fc82570246",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage = df.memory_usage(deep=True).sum()/(1024**2)\n",
    "print(f\"DataFrame size: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdba83c-da4b-496f-8538-22cf698ce99d",
   "metadata": {},
   "source": [
    "If we now that we only need a subset of variables, we can specify them in the dataset's `read()` method. \n",
    "\n",
    "For example, let's say that we are interested in the adjusted measurements of the dissolved oxygen recorded in the past 6 months in the NWA. To exclude non valid and missing observations, we filter the temperature (TEMP) to be within a (large) range of valid values. Besides the oxygen, we also want to keep the spatial and time coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb9ed7-2317-45c3-bb23-fa87f98623ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cols = [\"DB_NAME\",\"LATITUDE\",\"LONGITUDE\",\"PRES\",\"JULD\",\"TEMP\"]\n",
    "filter_coords_time_temp = [\n",
    "    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n",
    "    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n",
    "    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n",
    "    (\"TEMP\",\">=\",-1e30),(\"TEMP\",\"<=\",+1e30)\n",
    "]\n",
    "ds = pq.ParquetDataset(parquet_dir, schema=crocolake_schema, filters=filter_coords_time_temp)\n",
    "df = ds.read(columns=cols).to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df41724-e935-4a27-b93a-c7b20e16744c",
   "metadata": {},
   "source": [
    "This operation was much faster (3-4x faster on my machine) and loaded a smaller dataframe (~800 MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980da31d-9e0c-4b17-ab1e-88cd2647fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage = df.memory_usage(deep=True).sum()/(1024**2)\n",
    "print(f\"DataFrame size: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837eea39-65d9-4a9a-911b-8316f6a35b1f",
   "metadata": {},
   "source": [
    "#### Map\n",
    "\n",
    "We can now produce a scatter plot as we'd normally do with pandas. For example, here is the average dissolved oxygen at each location in the dataframe that we loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0365a-c8c1-4e98-aa51-a89071e9608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "ref_var = \"TEMP\"\n",
    "# Group by 'LATITUDE' and 'LONGITUDE' and database\n",
    "df_for_map = df.groupby([\"LATITUDE\", \"LONGITUDE\", \"DB_NAME\"]).agg({\n",
    "    ref_var: 'mean'  # Take the mean intensity at that coordinate\n",
    "}).reset_index()\n",
    "\n",
    "# Plotting using Cartopy\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "cbar_min = df_for_map[ref_var].quantile(q=0.05)\n",
    "cbar_max = df_for_map[ref_var].quantile(q=0.95)\n",
    "\n",
    "# Group by DB_NAME and plot each group\n",
    "df_for_map[\"DB_NAME\"] = pd.Categorical(df_for_map[\"DB_NAME\"], categories=[\"ARGO\",\"GLODAP\",\"SprayGliders\"], ordered=True)\n",
    "\n",
    "cmaps = iter( [\"Greens\",\"Oranges\",\"Blues\"] )\n",
    "for db_name, group in df_for_map.groupby(\"DB_NAME\",observed=False):\n",
    "    plt.scatter(\n",
    "        group['LONGITUDE'],\n",
    "        group['LATITUDE'],\n",
    "        s=10,\n",
    "        c=group[ref_var],\n",
    "        vmin=cbar_min,\n",
    "        vmax=cbar_max,\n",
    "        cmap=next(cmaps),\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "    plt.colorbar(shrink=0.5, label='Average ' + ref_var + \" (\" + db_name + \")\", pad=0.02)\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('North-West Atlantic average temperature 2010-2019 (Celsius)')\n",
    "plt.grid(True)\n",
    "plt.xlim([lon0, lon1])\n",
    "plt.ylim([lat0, lat1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589868a-c54a-45b1-bc97-63ecb78e096b",
   "metadata": {},
   "source": [
    "### dask approach\n",
    "\n",
    "dask can be faster at reading parquet databases thanks to its parallel and lazy evaluation of operations. \n",
    "\n",
    "When using dask, very few things change compared to using pandas and pyarrow: for example, dask dataframes are almost identical to pandas dataframes and indeed for our needs we will use the same syntax! The only difference is that dask does not evaluate the instructions immediately: it creates so-called delayed objects, through which it builds internally a graph of instructions optimized for the sequence of operations called. To trigger the actual computation we then need to call the `compute()` method of the dask dataframe. This allows dask to handle larger-than-memory datasets, by reading into memory only the portions needed to perform the optimized set of instructions.\n",
    "\n",
    "We start by importing a few extra modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621b451-9871-415a-ac99-e258fb515d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5095a2-595f-4ed9-9753-33e2c57a8053",
   "metadata": {
    "id": "zSiPCNSh4HBD"
   },
   "source": [
    "We can use the same filters defined earlier, and we use `read_parquet()` to filter the database and prescribe what we will need.\n",
    "Note that:\n",
    "* unlike the pyarrow-pandas approach, here we provide both filters and columns in the same method;\n",
    "* we provide the schema `crocolake_schema` that we read during the set up of the excercise\n",
    "* dask uses pyarrow under the hood (`engine` variable, other options are available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3860f70-f869-46d4-9bd2-526c289745d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed136e1c-8b38-4908-b745-0395174d1aa0",
    "outputId": "9cbe4727-8dc1-4c8c-f598-4dfa5b04338a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "date0 = datetime.datetime(2010, 1, 1, 0, 0, 0)\n",
    "date1 = datetime.datetime(2020, 1, 1, 0, 0, 0)\n",
    "\n",
    "cols = [\"DB_NAME\",\"LATITUDE\",\"LONGITUDE\",\"PRES\",\"JULD\",\"TEMP\"]\n",
    "filter_coords_time_temp = [\n",
    "    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n",
    "    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n",
    "    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n",
    "    (\"TEMP\",\">=\",-1e30),(\"TEMP\",\"<=\",+1e30)\n",
    "]\n",
    "\n",
    "ddf = dd.read_parquet(\n",
    "    parquet_dir,\n",
    "    engine=\"pyarrow\",\n",
    "    schema=crocolake_schema,\n",
    "    columns= cols,\n",
    "    filters=filter_coords_time_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab34ee-e8a9-4cdd-93e1-eeff24bce20a",
   "metadata": {
    "id": "M8LkMW-wQun5"
   },
   "source": [
    "It appears dask is much faster! Yet, it actually creates a delayed object, i.e. `ddf` contains the instructions that dask will later use to load the dataframe into memory. This allows us to use `ddf` to schedule all the operations that we'd normally perform on a dataframe. Eventually we will call `ddf.compute()` to actually evaluate the instructions.\n",
    "\n",
    "If we look at `ddf`, the dataframe will in fact appear empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2887793-e19d-450a-ae7e-3895c45b0ddf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "_wRG95P8Q3U1",
    "outputId": "81e3e10b-accb-45d1-8160-83f6c9db37da"
   },
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83833790-0f30-41ba-a3f8-7431739d75a9",
   "metadata": {
    "id": "uDAzYTduVA2B"
   },
   "source": [
    "We can explore its first entries with `head()`, which loads into memory only the first 5 entries:\n",
    "\n",
    "(Note: `head()` can return an empty dataframe even when `ddf` is not empty. This happens because `head()` looks for the first 5 rows of the first partition, which sometimes happens to be empty. Repartitioning the dask dataframe will get rid of empty partitions and solve this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e4d28-f1c6-4216-9d2f-194b02f52694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddf=ddf.repartition(partition_size=\"300MB\")\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0720b80-3a9a-4b93-9947-1a02c8ae4fca",
   "metadata": {
    "id": "W3p0MOcGVT_n"
   },
   "source": [
    "To load the whole dataframe, we call `compute()` (this returns a pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c4721-5729-4d82-8c1f-28c6804c665c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQsg5MOnVTZe",
    "outputId": "4ed38f6e-ebc3-4336-82b8-8554ba98f03c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_loaded = ddf.compute()\n",
    "ddf_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a781521-b793-4797-afcf-39a0d273c90f",
   "metadata": {
    "id": "v9T4CBmXWARf"
   },
   "source": [
    "It took just a few hundreds milliseconds! So yes, dask can be much faster than just using pyarrow and pandas.\n",
    "\n",
    "If we look into `ddf_loaded` now, it will show the populated pandas dataframe.\n",
    "\n",
    "(Note: if you compare `ddf_loaded` with `df` loaded with pyarrow, you'll see that the rows are not in the same order, yet under the dataframe you can see that the total number of rows is the same. Also the index seems smaller, but dask holds the dataframe in multiple partitions, and the index is reset at every partition.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019e472-862d-4976-8736-a3eac6b7ea09",
   "metadata": {},
   "source": [
    "dask's main advantage is not just in loading the data faster, but in performing operations on larger-than-memory data. For example, it can compute the mean value of TEMPERATURE *in the whole Argo PHY dataset* without loading the whole data in memory (and pretty quickly, too):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7cc89-58c6-498f-a559-3d7599a454e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we are not passing any filter conditions or selection of columns to load\n",
    "ddf_all = dd.read_parquet(\n",
    "    parquet_dir,\n",
    "    engine=\"pyarrow\",\n",
    "    schema=crocolake_schema,\n",
    ")\n",
    "temp_mean = ddf_all['TEMP'].mean() # temp_mean is a delayed object: it knows what operations to perform to compute the mean, \n",
    "                                   # but doesn't perform them until we call compute() in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42971d76-fb0b-431b-b4a1-f65a40c8a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "temp_mean.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e3bae-8810-4d92-b087-686e3dad1ac9",
   "metadata": {
    "id": "Kw7Ipj-KWjqv"
   },
   "source": [
    "#### Map\n",
    "\n",
    "When producing the map with our dask approach, we would create the `grouped` dataframe from the delayed dataframe `ddf`, and `compute()` it as late as possible.\n",
    "\n",
    "In this example, the compute that we are triggering includes four operations:\n",
    "* reading the filtered dataset;\n",
    "* grouping the dataframe by geographical coordinates;\n",
    "* averaging by pressure and time;\n",
    "* resetting the index;\n",
    "\n",
    "Dask internally builds a graph of all of these operations, so that it knows what subset of data is needed and it optimizes the number and sequence of instructions before executing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47af23-8bf0-4d9a-94f2-78575f30d184",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "62c816e6-bbd4-4af1-af47-03a1cfcbd95a",
    "outputId": "5319d870-15d4-4b08-d824-f37414392fe1"
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "ref_var = \"TEMP\"\n",
    "# Group by 'LATITUDE' and 'LONGITUDE' and database\n",
    "ddf_for_map = ddf.groupby([\"LATITUDE\", \"LONGITUDE\", \"DB_NAME\"]).agg({\n",
    "    ref_var: 'mean'  # Take the mean intensity at that coordinate\n",
    "}).reset_index()\n",
    "\n",
    "# Plotting using Cartopy\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "cbar_min = ddf_for_map[ref_var].quantile(q=0.05).compute()\n",
    "cbar_max = ddf_for_map[ref_var].quantile(q=0.95).compute()\n",
    "\n",
    "# Group by DB_NAME and plot each group\n",
    "ddf['DB_NAME'] = ddf.DB_NAME.astype('category')\n",
    "\n",
    "cmaps = iter( [\"Greens\",\"Oranges\",\"Blues\"] )\n",
    "for db_name, group in ddf_for_map.compute().groupby(\"DB_NAME\"):\n",
    "    plt.scatter(\n",
    "        group['LONGITUDE'],\n",
    "        group['LATITUDE'],\n",
    "        s=10,\n",
    "        c=group[ref_var],\n",
    "        vmin=cbar_min,\n",
    "        vmax=cbar_max,\n",
    "        cmap=next(cmaps),\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "    plt.colorbar(shrink=0.5, label='Average ' + ref_var + \" (\" + db_name + \")\", pad=0.02)\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('North-West Atlantic average temperature 2010-2019 (Celsius)')\n",
    "plt.grid(True)\n",
    "plt.xlim([lon0, lon1])\n",
    "plt.ylim([lat0, lat1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0033c65c-7cc4-4c7b-b85e-7d79c62ca8be",
   "metadata": {},
   "source": [
    "### Suggested exercises\n",
    "\n",
    "Try and access some other data, for example:\n",
    "* filtering by different time periods;\n",
    "* mapping a different parameter;\n",
    "* restraining the data of interest further by imposing the pressure PRES to be below 100db;\n",
    "* performing reads/manipulations that you would need to perform your tasks.\n",
    "\n",
    "If you encounter any issues, please [reach out](enrico.milanese@whoi.edu)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934aef4c-f28f-44a4-85d2-2908c3ff8e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
